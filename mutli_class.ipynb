{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import hamming_loss\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dic = {'inform_theater':0,'inform_starttime':1,'inform_numberofpeople':2,'greeting':3,'thanks':4,'inform_other':5,'request_moviename':6\n",
    "#             ,'inform_genre':7,'request_ticket':8,'inform_city':9,'inform_state':10,'inform_date':11,'inform_moviename':12,'confirm_answer':13,\n",
    "#             'inform_zip':14,'inform_video_format':15}\n",
    "class Config():\n",
    "    label_dic = {'inform_starttime':0,'inform_numberofpeople':1,'thanks':2,'confirm_answer':3}##设置抽取的类\n",
    "    embedding_size = 8  ##词编码维度\n",
    "    hidden_size = 8    ##隐藏层大小\n",
    "    label_dim = len(label_dic.keys())  #标签维度（标签的种类）\n",
    "    epoch = 100  #最大训练轮数\n",
    "    batch_size = 256  #每个batch的大小\n",
    "    lr = 1e-2 #学习率\n",
    "    ealy_stop = 3  #早停轮数（多少轮不下降就停止）\n",
    "    n_splits = 5 #多少折交叉验证\n",
    "    train = True ##训练标记，false不训练，使用现成模型预测\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x,length=10):##补全句子长度以及增加开头结尾\n",
    "    le = length-2\n",
    "    if len(x)>le:\n",
    "        x = x[0:8]\n",
    "    x.insert(0,'/u')\n",
    "    x.append('/s')\n",
    "    while len(x)<length:\n",
    "        x.append('/s')\n",
    "    return x\n",
    "\n",
    "def data_process(data_path,length=10):###输入文件路径，读取文件的句子及标签，返回句子列表，标签列表\n",
    "    sent_list = []\n",
    "    label_list = []\n",
    "    cnt = 0\n",
    "    with open(data_path,'r',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            cnt += 1\n",
    "            label_onehot = [0 for _ in range(len(config.label_dic.keys()))]\n",
    "            tmp = []\n",
    "            for item in line.split('\\t'):\n",
    "                tmp.append(item.replace('\\n',''))\n",
    "            label = []\n",
    "            for item in tmp:\n",
    "                if item in config.label_dic.keys():\n",
    "                    label_onehot[config.label_dic[item]] = 1\n",
    "            if sum(label_onehot)!=0:           \n",
    "                sent_list.append(pad(re.findall('[\\u4e00-\\u9fa5a-zA-Z0-9]+',tmp[0],re.S),length=length))##正则去除所有标点符号\n",
    "                label_list.append(label_onehot)\n",
    "    return sent_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent,train_label = data_process('train_data.tsv')##读取原始训练文件，返回切分好的句子，返回label onehot\n",
    "test_sent,test_label = data_process('test_data.tsv')##原始测试文件，返回切分好的句子，返回label onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###合并训练和测试文件，后续做五折交叉\n",
    "all_sent = []\n",
    "all_sent.extend(train_sent)\n",
    "all_sent.extend(test_sent)\n",
    "all_label = []\n",
    "all_label.extend(train_label)\n",
    "all_label.extend(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###使用word2vec预训练，并获取向量\n",
    "if config.train:\n",
    "    w2v = Word2Vec(all_sent,window=5,vector_size=config.embedding_size,min_count=1,seed=2021)\n",
    "    w2v.save('w2v.model')\n",
    "    w2v = Word2Vec.load('w2v.model')\n",
    "    weight = w2v.wv.vectors\n",
    "    vocab = w2v.wv.key_to_index \n",
    "else:\n",
    "    w2v = Word2Vec.load('w2v.model')##载入词向量模型\n",
    "    weight = w2v.wv.vectors\n",
    "    vocab = w2v.wv.key_to_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###将文字转换为词表的索引\n",
    "def vac2idx(lst,vocab):\n",
    "    tmp_list = []\n",
    "    for sent in lst:\n",
    "        s = []\n",
    "        for item in sent:\n",
    "            s.append(vocab[item])\n",
    "        tmp_list.append(s)\n",
    "    return tmp_list\n",
    "all_sent_idx = vac2idx(all_sent,vocab)\n",
    "# test_sent_idx = vac2idx(test_sent,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###句子索引及标签转换→tensor→dataset→迭代器\n",
    "def list2tensor(train_sent_idx,test_sent_idx,train_label,test_label):\n",
    "    train_set = torch.LongTensor(train_sent_idx)\n",
    "    test_set = torch.LongTensor(test_sent_idx)\n",
    "    train_label_set = torch.LongTensor(train_label)\n",
    "    test_label_set = torch.LongTensor(test_label)\n",
    "    train_set = TensorDataset(train_set,train_label_set)\n",
    "    test_set = TensorDataset(test_set,test_label_set)\n",
    "    train_iter = DataLoader(train_set,shuffle=True,batch_size=config.batch_size)\n",
    "    test_iter = DataLoader(test_set,shuffle=False,batch_size=config.batch_size)\n",
    "    return train_iter,test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###神经网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,weight):\n",
    "        super(Net,self).__init__()\n",
    "        self.weight = torch.FloatTensor(weight)\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.weight)\n",
    "        self.lstm = nn.LSTM(input_size=config.embedding_size,hidden_size=config.hidden_size,num_layers=2,bidirectional=True)\n",
    "        self.linear1 = nn.Linear(config.hidden_size*2,config.hidden_size)\n",
    "        self.linear2 = nn.Linear(config.hidden_size,config.label_dim)\n",
    "    def forward(self,x):\n",
    "        embed = self.embedding(x) ###embedding层\n",
    "        out, _ = self.lstm(embed) ###lstm层，out为lstm所有输出，维度，batch_size,句子长度，隐藏层维度*2\n",
    "        out = out.permute(0, 2, 1) ###转换维度为了下面的maxpool\n",
    "        out = F.adaptive_max_pool1d(out,output_size=1).squeeze()\n",
    "        x = self.linear1(out) ###dense+relu\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)##dense+sigmoid\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##定义损失函数\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "#####验证训练模型效果，返回验证集损失\n",
    "def evalute(data,net,label=None):\n",
    "    loss_g = 0\n",
    "    cnt = 0\n",
    "    hm_loss_tot = 0\n",
    "    for i, (trains, labels) in enumerate(data):\n",
    "        cnt += trains.shape[0]\n",
    "        outputs = net(trains)\n",
    "        hm_loss = hamming_loss(np.round(outputs.data.numpy()),labels.numpy())##汉明损失\n",
    "        hm_loss_tot += hm_loss * trains.shape[0]\n",
    "        net.zero_grad()\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        loss_g += loss\n",
    "    return loss_g,hm_loss_tot/cnt##返回bce损失和汉明损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "####定义多折切分器\n",
    "kfold = KFold(n_splits=config.n_splits,random_state=2021,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###根据索引返回训练集和验证集\n",
    "def get_data(data,label,train_idx,test_idx):\n",
    "    data_train = [data[_] for _ in train_idx]\n",
    "    data_test = [data[_] for _ in test_idx]\n",
    "    label_train = [label[_] for _ in train_idx]\n",
    "    label_test = [label[_] for _ in test_idx]\n",
    "    return data_train,data_test,label_train,label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:428: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = (n_samples // n_splits) * np.ones(n_splits, dtype=np.int)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss:inf current loss:2.9504072666168213 epoch:0,k_flod:0\n",
      "best loss:2.9504072666168213 current loss:2.9145865440368652 epoch:1,k_flod:0\n",
      "best loss:2.9145865440368652 current loss:2.9085423946380615 epoch:2,k_flod:0\n",
      "best loss:2.9085423946380615 current loss:2.833122730255127 epoch:3,k_flod:0\n",
      "best loss:2.833122730255127 current loss:2.568243980407715 epoch:4,k_flod:0\n",
      "best loss:2.568243980407715 current loss:2.39871883392334 epoch:5,k_flod:0\n",
      "best loss:2.39871883392334 current loss:2.2442452907562256 epoch:6,k_flod:0\n",
      "best loss:2.2442452907562256 current loss:2.111478328704834 epoch:7,k_flod:0\n",
      "best loss:2.111478328704834 current loss:1.9757719039916992 epoch:8,k_flod:0\n",
      "best loss:1.9757719039916992 current loss:1.834974765777588 epoch:9,k_flod:0\n",
      "best loss:1.834974765777588 current loss:1.7347509860992432 epoch:10,k_flod:0\n",
      "best loss:1.7347509860992432 current loss:1.6789418458938599 epoch:11,k_flod:0\n",
      "best loss:1.6789418458938599 current loss:1.6295115947723389 epoch:12,k_flod:0\n",
      "best loss:1.6295115947723389 current loss:1.6014312505722046 epoch:13,k_flod:0\n",
      "best loss:1.6014312505722046 current loss:1.5791196823120117 epoch:14,k_flod:0\n",
      "best loss:1.5791196823120117 current loss:1.5454401969909668 epoch:15,k_flod:0\n",
      "best loss:1.5454401969909668 current loss:1.5557780265808105 epoch:16,k_flod:0\n",
      "best loss:1.5454401969909668 current loss:1.5078673362731934 epoch:17,k_flod:0\n",
      "best loss:1.5078673362731934 current loss:1.4876965284347534 epoch:18,k_flod:0\n",
      "best loss:1.4876965284347534 current loss:1.477642297744751 epoch:19,k_flod:0\n",
      "best loss:1.477642297744751 current loss:1.4720838069915771 epoch:20,k_flod:0\n",
      "best loss:1.4720838069915771 current loss:1.4497500658035278 epoch:21,k_flod:0\n",
      "best loss:1.4497500658035278 current loss:1.4275612831115723 epoch:22,k_flod:0\n",
      "best loss:1.4275612831115723 current loss:1.3986232280731201 epoch:23,k_flod:0\n",
      "best loss:1.3986232280731201 current loss:1.3885294198989868 epoch:24,k_flod:0\n",
      "best loss:1.3885294198989868 current loss:1.3808810710906982 epoch:25,k_flod:0\n",
      "best loss:1.3808810710906982 current loss:1.3848751783370972 epoch:26,k_flod:0\n",
      "best loss:1.3808810710906982 current loss:1.3442412614822388 epoch:27,k_flod:0\n",
      "best loss:1.3442412614822388 current loss:1.3606013059616089 epoch:28,k_flod:0\n",
      "best loss:1.3442412614822388 current loss:1.3374594449996948 epoch:29,k_flod:0\n",
      "best loss:1.3374594449996948 current loss:1.3161721229553223 epoch:30,k_flod:0\n",
      "best loss:1.3161721229553223 current loss:1.307626724243164 epoch:31,k_flod:0\n",
      "best loss:1.307626724243164 current loss:1.3113315105438232 epoch:32,k_flod:0\n",
      "best loss:1.307626724243164 current loss:1.289937973022461 epoch:33,k_flod:0\n",
      "best loss:1.289937973022461 current loss:1.287056803703308 epoch:34,k_flod:0\n",
      "best loss:1.287056803703308 current loss:1.3003690242767334 epoch:35,k_flod:0\n",
      "best loss:1.287056803703308 current loss:1.27494215965271 epoch:36,k_flod:0\n",
      "best loss:1.27494215965271 current loss:1.2798542976379395 epoch:37,k_flod:0\n",
      "best loss:1.27494215965271 current loss:1.2677619457244873 epoch:38,k_flod:0\n",
      "best loss:1.2677619457244873 current loss:1.2485661506652832 epoch:39,k_flod:0\n",
      "best loss:1.2485661506652832 current loss:1.234025001525879 epoch:40,k_flod:0\n",
      "best loss:1.234025001525879 current loss:1.2219926118850708 epoch:41,k_flod:0\n",
      "best loss:1.2219926118850708 current loss:1.2123571634292603 epoch:42,k_flod:0\n",
      "best loss:1.2123571634292603 current loss:1.2075932025909424 epoch:43,k_flod:0\n",
      "best loss:1.2075932025909424 current loss:1.194295883178711 epoch:44,k_flod:0\n",
      "best loss:1.194295883178711 current loss:1.1989178657531738 epoch:45,k_flod:0\n",
      "best loss:1.194295883178711 current loss:1.1843147277832031 epoch:46,k_flod:0\n",
      "best loss:1.1843147277832031 current loss:1.189122200012207 epoch:47,k_flod:0\n",
      "best loss:1.1843147277832031 current loss:1.185239553451538 epoch:48,k_flod:0\n",
      "best loss:1.1843147277832031 current loss:1.1715378761291504 epoch:49,k_flod:0\n",
      "best loss:1.1715378761291504 current loss:1.1678906679153442 epoch:50,k_flod:0\n",
      "best loss:1.1678906679153442 current loss:1.1597970724105835 epoch:51,k_flod:0\n",
      "best loss:1.1597970724105835 current loss:1.1600055694580078 epoch:52,k_flod:0\n",
      "best loss:1.1597970724105835 current loss:1.155747890472412 epoch:53,k_flod:0\n",
      "best loss:1.155747890472412 current loss:1.1551265716552734 epoch:54,k_flod:0\n",
      "best loss:1.1551265716552734 current loss:1.148918867111206 epoch:55,k_flod:0\n",
      "best loss:1.148918867111206 current loss:1.1569194793701172 epoch:56,k_flod:0\n",
      "best loss:1.148918867111206 current loss:1.1415119171142578 epoch:57,k_flod:0\n",
      "best loss:1.1415119171142578 current loss:1.1403981447219849 epoch:58,k_flod:0\n",
      "best loss:1.1403981447219849 current loss:1.1428945064544678 epoch:59,k_flod:0\n",
      "best loss:1.1403981447219849 current loss:1.1568927764892578 epoch:60,k_flod:0\n",
      "best loss:1.1403981447219849 current loss:1.1422396898269653 epoch:61,k_flod:0\n",
      "第0折汉明损失：0.09876543209876543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss:inf current loss:2.961393117904663 epoch:0,k_flod:1\n",
      "best loss:2.961393117904663 current loss:2.870081901550293 epoch:1,k_flod:1\n",
      "best loss:2.870081901550293 current loss:2.603938102722168 epoch:2,k_flod:1\n",
      "best loss:2.603938102722168 current loss:2.3023524284362793 epoch:3,k_flod:1\n",
      "best loss:2.3023524284362793 current loss:2.1656265258789062 epoch:4,k_flod:1\n",
      "best loss:2.1656265258789062 current loss:2.0713157653808594 epoch:5,k_flod:1\n",
      "best loss:2.0713157653808594 current loss:1.9681131839752197 epoch:6,k_flod:1\n",
      "best loss:1.9681131839752197 current loss:1.7933900356292725 epoch:7,k_flod:1\n",
      "best loss:1.7933900356292725 current loss:1.7013081312179565 epoch:8,k_flod:1\n",
      "best loss:1.7013081312179565 current loss:1.6241227388381958 epoch:9,k_flod:1\n",
      "best loss:1.6241227388381958 current loss:1.5298504829406738 epoch:10,k_flod:1\n",
      "best loss:1.5298504829406738 current loss:1.4767357110977173 epoch:11,k_flod:1\n",
      "best loss:1.4767357110977173 current loss:1.4600605964660645 epoch:12,k_flod:1\n",
      "best loss:1.4600605964660645 current loss:1.5785467624664307 epoch:13,k_flod:1\n",
      "best loss:1.4600605964660645 current loss:1.4007344245910645 epoch:14,k_flod:1\n",
      "best loss:1.4007344245910645 current loss:1.3840440511703491 epoch:15,k_flod:1\n",
      "best loss:1.3840440511703491 current loss:1.3616691827774048 epoch:16,k_flod:1\n",
      "best loss:1.3616691827774048 current loss:1.3472630977630615 epoch:17,k_flod:1\n",
      "best loss:1.3472630977630615 current loss:1.3239952325820923 epoch:18,k_flod:1\n",
      "best loss:1.3239952325820923 current loss:1.3125503063201904 epoch:19,k_flod:1\n",
      "best loss:1.3125503063201904 current loss:1.2954200506210327 epoch:20,k_flod:1\n",
      "best loss:1.2954200506210327 current loss:1.263358473777771 epoch:21,k_flod:1\n",
      "best loss:1.263358473777771 current loss:1.2537137269973755 epoch:22,k_flod:1\n",
      "best loss:1.2537137269973755 current loss:1.2475945949554443 epoch:23,k_flod:1\n",
      "best loss:1.2475945949554443 current loss:1.231081247329712 epoch:24,k_flod:1\n",
      "best loss:1.231081247329712 current loss:1.2472052574157715 epoch:25,k_flod:1\n",
      "best loss:1.231081247329712 current loss:1.2211997509002686 epoch:26,k_flod:1\n",
      "best loss:1.2211997509002686 current loss:1.218774676322937 epoch:27,k_flod:1\n",
      "best loss:1.218774676322937 current loss:1.1569904088974 epoch:28,k_flod:1\n",
      "best loss:1.1569904088974 current loss:1.1432545185089111 epoch:29,k_flod:1\n",
      "best loss:1.1432545185089111 current loss:1.110934853553772 epoch:30,k_flod:1\n",
      "best loss:1.110934853553772 current loss:1.1108241081237793 epoch:31,k_flod:1\n",
      "best loss:1.1108241081237793 current loss:1.0803194046020508 epoch:32,k_flod:1\n",
      "best loss:1.0803194046020508 current loss:1.0683823823928833 epoch:33,k_flod:1\n",
      "best loss:1.0683823823928833 current loss:1.060957908630371 epoch:34,k_flod:1\n",
      "best loss:1.060957908630371 current loss:1.0630221366882324 epoch:35,k_flod:1\n",
      "best loss:1.060957908630371 current loss:1.0407787561416626 epoch:36,k_flod:1\n",
      "best loss:1.0407787561416626 current loss:1.0438183546066284 epoch:37,k_flod:1\n",
      "best loss:1.0407787561416626 current loss:1.030734658241272 epoch:38,k_flod:1\n",
      "best loss:1.030734658241272 current loss:1.0292633771896362 epoch:39,k_flod:1\n",
      "best loss:1.0292633771896362 current loss:1.0287400484085083 epoch:40,k_flod:1\n",
      "best loss:1.0287400484085083 current loss:1.0347932577133179 epoch:41,k_flod:1\n",
      "best loss:1.0287400484085083 current loss:1.019551396369934 epoch:42,k_flod:1\n",
      "best loss:1.019551396369934 current loss:1.0128920078277588 epoch:43,k_flod:1\n",
      "best loss:1.0128920078277588 current loss:1.0202748775482178 epoch:44,k_flod:1\n",
      "best loss:1.0128920078277588 current loss:1.015459656715393 epoch:45,k_flod:1\n",
      "best loss:1.0128920078277588 current loss:1.0215741395950317 epoch:46,k_flod:1\n",
      "第1折汉明损失：0.08664021164021164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss:inf current loss:2.999257802963257 epoch:0,k_flod:2\n",
      "best loss:2.999257802963257 current loss:2.9136204719543457 epoch:1,k_flod:2\n",
      "best loss:2.9136204719543457 current loss:2.6956140995025635 epoch:2,k_flod:2\n",
      "best loss:2.6956140995025635 current loss:2.360269069671631 epoch:3,k_flod:2\n",
      "best loss:2.360269069671631 current loss:2.258441925048828 epoch:4,k_flod:2\n",
      "best loss:2.258441925048828 current loss:2.2448697090148926 epoch:5,k_flod:2\n",
      "best loss:2.2448697090148926 current loss:2.2274999618530273 epoch:6,k_flod:2\n",
      "best loss:2.2274999618530273 current loss:2.1813509464263916 epoch:7,k_flod:2\n",
      "best loss:2.1813509464263916 current loss:2.1746082305908203 epoch:8,k_flod:2\n",
      "best loss:2.1746082305908203 current loss:2.1321403980255127 epoch:9,k_flod:2\n",
      "best loss:2.1321403980255127 current loss:2.0488622188568115 epoch:10,k_flod:2\n",
      "best loss:2.0488622188568115 current loss:1.9963557720184326 epoch:11,k_flod:2\n",
      "best loss:1.9963557720184326 current loss:1.942668080329895 epoch:12,k_flod:2\n",
      "best loss:1.942668080329895 current loss:1.885770320892334 epoch:13,k_flod:2\n",
      "best loss:1.885770320892334 current loss:1.661102533340454 epoch:14,k_flod:2\n",
      "best loss:1.661102533340454 current loss:1.587996006011963 epoch:15,k_flod:2\n",
      "best loss:1.587996006011963 current loss:1.5191245079040527 epoch:16,k_flod:2\n",
      "best loss:1.5191245079040527 current loss:1.4730885028839111 epoch:17,k_flod:2\n",
      "best loss:1.4730885028839111 current loss:1.3621999025344849 epoch:18,k_flod:2\n",
      "best loss:1.3621999025344849 current loss:1.29646635055542 epoch:19,k_flod:2\n",
      "best loss:1.29646635055542 current loss:1.3138350248336792 epoch:20,k_flod:2\n",
      "best loss:1.29646635055542 current loss:1.246498465538025 epoch:21,k_flod:2\n",
      "best loss:1.246498465538025 current loss:1.235802412033081 epoch:22,k_flod:2\n",
      "best loss:1.235802412033081 current loss:1.2055920362472534 epoch:23,k_flod:2\n",
      "best loss:1.2055920362472534 current loss:1.1997562646865845 epoch:24,k_flod:2\n",
      "best loss:1.1997562646865845 current loss:1.1836985349655151 epoch:25,k_flod:2\n",
      "best loss:1.1836985349655151 current loss:1.1546101570129395 epoch:26,k_flod:2\n",
      "best loss:1.1546101570129395 current loss:1.1397448778152466 epoch:27,k_flod:2\n",
      "best loss:1.1397448778152466 current loss:1.1207184791564941 epoch:28,k_flod:2\n",
      "best loss:1.1207184791564941 current loss:1.121368408203125 epoch:29,k_flod:2\n",
      "best loss:1.1207184791564941 current loss:1.1150224208831787 epoch:30,k_flod:2\n",
      "best loss:1.1150224208831787 current loss:1.1258074045181274 epoch:31,k_flod:2\n",
      "best loss:1.1150224208831787 current loss:1.1069295406341553 epoch:32,k_flod:2\n",
      "best loss:1.1069295406341553 current loss:1.122801423072815 epoch:33,k_flod:2\n",
      "best loss:1.1069295406341553 current loss:1.1053521633148193 epoch:34,k_flod:2\n",
      "best loss:1.1053521633148193 current loss:1.1036211252212524 epoch:35,k_flod:2\n",
      "best loss:1.1036211252212524 current loss:1.0886106491088867 epoch:36,k_flod:2\n",
      "best loss:1.0886106491088867 current loss:1.0892579555511475 epoch:37,k_flod:2\n",
      "best loss:1.0886106491088867 current loss:1.0942963361740112 epoch:38,k_flod:2\n",
      "best loss:1.0886106491088867 current loss:1.0959748029708862 epoch:39,k_flod:2\n",
      "第2折汉明损失：0.09068843777581642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss:inf current loss:3.030233383178711 epoch:0,k_flod:3\n",
      "best loss:3.030233383178711 current loss:2.8751840591430664 epoch:1,k_flod:3\n",
      "best loss:2.8751840591430664 current loss:2.8849453926086426 epoch:2,k_flod:3\n",
      "best loss:2.8751840591430664 current loss:2.85640811920166 epoch:3,k_flod:3\n",
      "best loss:2.85640811920166 current loss:2.7817230224609375 epoch:4,k_flod:3\n",
      "best loss:2.7817230224609375 current loss:2.541343927383423 epoch:5,k_flod:3\n",
      "best loss:2.541343927383423 current loss:2.3765580654144287 epoch:6,k_flod:3\n",
      "best loss:2.3765580654144287 current loss:2.269014596939087 epoch:7,k_flod:3\n",
      "best loss:2.269014596939087 current loss:2.1248726844787598 epoch:8,k_flod:3\n",
      "best loss:2.1248726844787598 current loss:2.0034070014953613 epoch:9,k_flod:3\n",
      "best loss:2.0034070014953613 current loss:1.8757959604263306 epoch:10,k_flod:3\n",
      "best loss:1.8757959604263306 current loss:1.7639520168304443 epoch:11,k_flod:3\n",
      "best loss:1.7639520168304443 current loss:1.6587669849395752 epoch:12,k_flod:3\n",
      "best loss:1.6587669849395752 current loss:1.6056543588638306 epoch:13,k_flod:3\n",
      "best loss:1.6056543588638306 current loss:1.576870322227478 epoch:14,k_flod:3\n",
      "best loss:1.576870322227478 current loss:1.5523052215576172 epoch:15,k_flod:3\n",
      "best loss:1.5523052215576172 current loss:1.5459346771240234 epoch:16,k_flod:3\n",
      "best loss:1.5459346771240234 current loss:1.5306357145309448 epoch:17,k_flod:3\n",
      "best loss:1.5306357145309448 current loss:1.5214250087738037 epoch:18,k_flod:3\n",
      "best loss:1.5214250087738037 current loss:1.5212795734405518 epoch:19,k_flod:3\n",
      "best loss:1.5212795734405518 current loss:1.4977035522460938 epoch:20,k_flod:3\n",
      "best loss:1.4977035522460938 current loss:1.4957289695739746 epoch:21,k_flod:3\n",
      "best loss:1.4957289695739746 current loss:1.4856773614883423 epoch:22,k_flod:3\n",
      "best loss:1.4856773614883423 current loss:1.4679828882217407 epoch:23,k_flod:3\n",
      "best loss:1.4679828882217407 current loss:1.4509837627410889 epoch:24,k_flod:3\n",
      "best loss:1.4509837627410889 current loss:1.468039631843567 epoch:25,k_flod:3\n",
      "best loss:1.4509837627410889 current loss:1.4334843158721924 epoch:26,k_flod:3\n",
      "best loss:1.4334843158721924 current loss:1.4087707996368408 epoch:27,k_flod:3\n",
      "best loss:1.4087707996368408 current loss:1.4165900945663452 epoch:28,k_flod:3\n",
      "best loss:1.4087707996368408 current loss:1.3783178329467773 epoch:29,k_flod:3\n",
      "best loss:1.3783178329467773 current loss:1.373036503791809 epoch:30,k_flod:3\n",
      "best loss:1.373036503791809 current loss:1.3506593704223633 epoch:31,k_flod:3\n",
      "best loss:1.3506593704223633 current loss:1.3451099395751953 epoch:32,k_flod:3\n",
      "best loss:1.3451099395751953 current loss:1.3218071460723877 epoch:33,k_flod:3\n",
      "best loss:1.3218071460723877 current loss:1.3211102485656738 epoch:34,k_flod:3\n",
      "best loss:1.3211102485656738 current loss:1.2984542846679688 epoch:35,k_flod:3\n",
      "best loss:1.2984542846679688 current loss:1.2946045398712158 epoch:36,k_flod:3\n",
      "best loss:1.2946045398712158 current loss:1.2949663400650024 epoch:37,k_flod:3\n",
      "best loss:1.2946045398712158 current loss:1.2794005870819092 epoch:38,k_flod:3\n",
      "best loss:1.2794005870819092 current loss:1.3159263134002686 epoch:39,k_flod:3\n",
      "best loss:1.2794005870819092 current loss:1.2869179248809814 epoch:40,k_flod:3\n",
      "best loss:1.2794005870819092 current loss:1.2704373598098755 epoch:41,k_flod:3\n",
      "best loss:1.2704373598098755 current loss:1.2626420259475708 epoch:42,k_flod:3\n",
      "best loss:1.2626420259475708 current loss:1.2663283348083496 epoch:43,k_flod:3\n",
      "best loss:1.2626420259475708 current loss:1.2476574182510376 epoch:44,k_flod:3\n",
      "best loss:1.2476574182510376 current loss:1.249770164489746 epoch:45,k_flod:3\n",
      "best loss:1.2476574182510376 current loss:1.2590700387954712 epoch:46,k_flod:3\n",
      "best loss:1.2476574182510376 current loss:1.2397555112838745 epoch:47,k_flod:3\n",
      "best loss:1.2397555112838745 current loss:1.233642339706421 epoch:48,k_flod:3\n",
      "best loss:1.233642339706421 current loss:1.2281105518341064 epoch:49,k_flod:3\n",
      "best loss:1.2281105518341064 current loss:1.2234001159667969 epoch:50,k_flod:3\n",
      "best loss:1.2234001159667969 current loss:1.231243371963501 epoch:51,k_flod:3\n",
      "best loss:1.2234001159667969 current loss:1.2243698835372925 epoch:52,k_flod:3\n",
      "best loss:1.2234001159667969 current loss:1.2202446460723877 epoch:53,k_flod:3\n",
      "best loss:1.2202446460723877 current loss:1.2277206182479858 epoch:54,k_flod:3\n",
      "best loss:1.2202446460723877 current loss:1.2149171829223633 epoch:55,k_flod:3\n",
      "best loss:1.2149171829223633 current loss:1.2118268013000488 epoch:56,k_flod:3\n",
      "best loss:1.2118268013000488 current loss:1.2051211595535278 epoch:57,k_flod:3\n",
      "best loss:1.2051211595535278 current loss:1.2093532085418701 epoch:58,k_flod:3\n",
      "best loss:1.2051211595535278 current loss:1.2067294120788574 epoch:59,k_flod:3\n",
      "best loss:1.2051211595535278 current loss:1.2012385129928589 epoch:60,k_flod:3\n",
      "best loss:1.2012385129928589 current loss:1.2043473720550537 epoch:61,k_flod:3\n",
      "best loss:1.2012385129928589 current loss:1.213317632675171 epoch:62,k_flod:3\n",
      "best loss:1.2012385129928589 current loss:1.2045567035675049 epoch:63,k_flod:3\n",
      "第3折汉明损失：0.10039717563989409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss:inf current loss:2.928699016571045 epoch:0,k_flod:4\n",
      "best loss:2.928699016571045 current loss:2.908864974975586 epoch:1,k_flod:4\n",
      "best loss:2.908864974975586 current loss:2.902780532836914 epoch:2,k_flod:4\n",
      "best loss:2.902780532836914 current loss:2.863394021987915 epoch:3,k_flod:4\n",
      "best loss:2.863394021987915 current loss:2.641883134841919 epoch:4,k_flod:4\n",
      "best loss:2.641883134841919 current loss:2.3492140769958496 epoch:5,k_flod:4\n",
      "best loss:2.3492140769958496 current loss:2.2401745319366455 epoch:6,k_flod:4\n",
      "best loss:2.2401745319366455 current loss:2.1738576889038086 epoch:7,k_flod:4\n",
      "best loss:2.1738576889038086 current loss:2.1339406967163086 epoch:8,k_flod:4\n",
      "best loss:2.1339406967163086 current loss:2.082793712615967 epoch:9,k_flod:4\n",
      "best loss:2.082793712615967 current loss:2.022919178009033 epoch:10,k_flod:4\n",
      "best loss:2.022919178009033 current loss:1.9482300281524658 epoch:11,k_flod:4\n",
      "best loss:1.9482300281524658 current loss:1.8376824855804443 epoch:12,k_flod:4\n",
      "best loss:1.8376824855804443 current loss:1.762380838394165 epoch:13,k_flod:4\n",
      "best loss:1.762380838394165 current loss:1.6751948595046997 epoch:14,k_flod:4\n",
      "best loss:1.6751948595046997 current loss:1.6051173210144043 epoch:15,k_flod:4\n",
      "best loss:1.6051173210144043 current loss:1.5495078563690186 epoch:16,k_flod:4\n",
      "best loss:1.5495078563690186 current loss:1.4764679670333862 epoch:17,k_flod:4\n",
      "best loss:1.4764679670333862 current loss:1.4165046215057373 epoch:18,k_flod:4\n",
      "best loss:1.4165046215057373 current loss:1.3596651554107666 epoch:19,k_flod:4\n",
      "best loss:1.3596651554107666 current loss:1.3166078329086304 epoch:20,k_flod:4\n",
      "best loss:1.3166078329086304 current loss:1.2615199089050293 epoch:21,k_flod:4\n",
      "best loss:1.2615199089050293 current loss:1.2486021518707275 epoch:22,k_flod:4\n",
      "best loss:1.2486021518707275 current loss:1.204329490661621 epoch:23,k_flod:4\n",
      "best loss:1.204329490661621 current loss:1.237221121788025 epoch:24,k_flod:4\n",
      "best loss:1.204329490661621 current loss:1.1870709657669067 epoch:25,k_flod:4\n",
      "best loss:1.1870709657669067 current loss:1.1684690713882446 epoch:26,k_flod:4\n",
      "best loss:1.1684690713882446 current loss:1.1662009954452515 epoch:27,k_flod:4\n",
      "best loss:1.1662009954452515 current loss:1.1755470037460327 epoch:28,k_flod:4\n",
      "best loss:1.1662009954452515 current loss:1.149821400642395 epoch:29,k_flod:4\n",
      "best loss:1.149821400642395 current loss:1.140655755996704 epoch:30,k_flod:4\n",
      "best loss:1.140655755996704 current loss:1.137243390083313 epoch:31,k_flod:4\n",
      "best loss:1.137243390083313 current loss:1.1463197469711304 epoch:32,k_flod:4\n",
      "best loss:1.137243390083313 current loss:1.1346282958984375 epoch:33,k_flod:4\n",
      "best loss:1.1346282958984375 current loss:1.1169447898864746 epoch:34,k_flod:4\n",
      "best loss:1.1169447898864746 current loss:1.1161880493164062 epoch:35,k_flod:4\n",
      "best loss:1.1161880493164062 current loss:1.1158108711242676 epoch:36,k_flod:4\n",
      "best loss:1.1158108711242676 current loss:1.1023224592208862 epoch:37,k_flod:4\n",
      "best loss:1.1023224592208862 current loss:1.1205756664276123 epoch:38,k_flod:4\n",
      "best loss:1.1023224592208862 current loss:1.102048635482788 epoch:39,k_flod:4\n",
      "best loss:1.102048635482788 current loss:1.1088330745697021 epoch:40,k_flod:4\n",
      "best loss:1.102048635482788 current loss:1.098103642463684 epoch:41,k_flod:4\n",
      "best loss:1.098103642463684 current loss:1.0982410907745361 epoch:42,k_flod:4\n",
      "best loss:1.098103642463684 current loss:1.0801441669464111 epoch:43,k_flod:4\n",
      "best loss:1.0801441669464111 current loss:1.0793180465698242 epoch:44,k_flod:4\n",
      "best loss:1.0793180465698242 current loss:1.0874900817871094 epoch:45,k_flod:4\n",
      "best loss:1.0793180465698242 current loss:1.0838477611541748 epoch:46,k_flod:4\n",
      "best loss:1.0793180465698242 current loss:1.0838068723678589 epoch:47,k_flod:4\n",
      "第4折汉明损失：0.0942188879082083\n",
      "五折平均汉明损失： 0.09414202901257918\n"
     ]
    }
   ],
   "source": [
    "#模型训练\n",
    "if config.train:\n",
    "    hm_loss_list = [] #储存每折的汉明损失\n",
    "    for k, (train_index, test_index) in enumerate(kfold.split(all_sent_idx)):   \n",
    "        train_sent_idx,test_sent_idx,train_label,test_label = get_data(all_sent_idx,all_label,train_index,test_index)##切分训练集和验证集\n",
    "        train_iter,test_iter = list2tensor(train_sent_idx,test_sent_idx,train_label,test_label)##返回迭代器\n",
    "        net = Net(weight) #初始化网络\n",
    "        net.train() #开启训练\n",
    "        optimizer = optim.Adam(net.parameters(),lr=config.lr) #定义优化器\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) #减少学习速率\n",
    "        dev_best_loss = float('inf')\n",
    "        cnt = 0 #loss没有下降的轮数\n",
    "        hm_loss_best = 0\n",
    "        for epoch in range(config.epoch):\n",
    "            loss_g = 0\n",
    "            for i, (trains, labels) in enumerate(train_iter):\n",
    "                outputs = net(trains)\n",
    "                loss = loss_fn(outputs, labels.float())\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_g += loss\n",
    "            loss_dev,hm_loss = evalute(test_iter,net) ##验证集输出损失，汉明损失\n",
    "            print('best loss:{} current loss:{} epoch:{},k_flod:{}'.format(dev_best_loss,loss_dev,epoch,k))\n",
    "            if loss_dev<dev_best_loss:##损失减小，保存模型\n",
    "                hm_loss_best = hm_loss\n",
    "                dev_best_loss = loss_dev\n",
    "                torch.save(net.state_dict(),'./model/model{}.pth'.format(k))\n",
    "                cnt = 0 \n",
    "            else:\n",
    "                scheduler.step()\n",
    "                cnt += 1\n",
    "            if cnt >= config.ealy_stop:\n",
    "                print('第{}折汉明损失：{}'.format(k,hm_loss_best))\n",
    "                break\n",
    "        hm_loss_list.append(hm_loss_best)\n",
    "    print('五折平均汉明损失：',sum(hm_loss_list)/len(hm_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##以下用来自行测试数据\n",
    "net = Net(weight)\n",
    "\n",
    "def predict(sent,model_path,net):##输入句子，模型路径，网络\n",
    "    test_data = test_data_process(sent)\n",
    "    sent_idx = vac2idx(test_data,vocab)\n",
    "    sent = torch.LongTensor(sent_idx)    \n",
    "    model_file = os.listdir(model_path)\n",
    "    output_list = []\n",
    "    for model in model_file:\n",
    "        net.load_state_dict(torch.load(os.path.join(model_path,model)))\n",
    "        output_list.append(net(sent).data.numpy())\n",
    "    out = np.zeros_like(output_list[0])\n",
    "    for output in output_list:\n",
    "        out += output\n",
    "    return out/len(output_list)\n",
    "        \n",
    "def test_data_process(sents,length=10):  ##返回切分后的句子\n",
    "    sent_list = []\n",
    "    for sent in sents:\n",
    "        sent_list.append(pad(re.findall('[\\u4e00-\\u9fa5a-zA-Z0-9]+',sent,re.S),length=length))##正则去除所有标点符号\n",
    "    return sent_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####sent为你想测试的句子\n",
    "sent = ['thank you so much!','930 would be perfect: 4 tickets?']\n",
    "out = predict(sent,'model',net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个句子的预测结果\n",
      "thanks\n",
      "第1个句子的预测结果\n",
      "inform_starttime\n",
      "inform_numberofpeople\n"
     ]
    }
   ],
   "source": [
    "for i in range(out.shape[0]):\n",
    "    print('第{}个句子的预测结果'.format(i))\n",
    "    for j in range(out.shape[1]):\n",
    "        if out[i][j]>0.5:\n",
    "            print(list(config.label_dic.keys())[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
